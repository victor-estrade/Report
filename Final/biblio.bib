@article{Ganin2016,
abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective do-main transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target do-main (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gra-dient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the ap-proach for descriptor learning task in the context of person re-identification application.},
archivePrefix = {arXiv},
arxivId = {1505.07818v1},
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor and Dogan, Urun and Kloft, Marius and Orabona, Francesco and Tommasi, Tatiana},
doi = {10.1088/1475-7516/2015/08/013},
eprint = {1505.07818v1},
file = {:home/victor/Dropbox/Mendeley/PDF/Ganin et al. - 2016 - Domain-Adversarial Training of Neural Networks.pdf:pdf},
issn = {1475-7516},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,domain adaptation,image classification,neural network,person re-identification,representation learning,sentiment analysis,synthetic data},
pages = {1--35},
title = {{Domain-Adversarial Training of Neural Networks}},
volume = {17},
year = {2016}
}
@article{Ben-David2010,
abstract = {Discriminative learning methods for classification perform well when$\backslash$ntraining and test data are drawn from the same distribution. Often,$\backslash$nhowever, we have plentiful labeled training data from a source domain$\backslash$nbut wish to learn a classifier which performs well on a target domain$\backslash$nwith a different distribution and little or no labeled training data. In$\backslash$nthis work we investigate two questions. First, under what conditions can$\backslash$na classifier trained from source data be expected to perform well on$\backslash$ntarget data? Second, given a small amount of labeled target data, how$\backslash$nshould we combine it during training with the large amount of labeled$\backslash$nsource data to achieve the lowest target error at test time?$\backslash$nWe address the first question by bounding a classifier's target error in$\backslash$nterms of its source error and the divergence between the two domains. We$\backslash$ngive a classifier-induced divergence measure that can be estimated from$\backslash$nfinite, unlabeled samples from the domains. Under the assumption that$\backslash$nthere exists some hypothesis that performs well in both domains, we show$\backslash$nthat this quantity together with the empirical source error characterize$\backslash$nthe target error of a source-trained classifier.$\backslash$nWe answer the second question by bounding the target error of a model$\backslash$nwhich minimizes a convex combination of the empirical source and target$\backslash$nerrors. Previous theoretical work has considered minimizing just the$\backslash$nsource error, just the target error, or weighting instances from the two$\backslash$ndomains equally. We show how to choose the optimal combination of source$\backslash$nand target error as a function of the divergence, the sample sizes of$\backslash$nboth domains, and the complexity of the hypothesis class. The resulting$\backslash$nbound generalizes the previously studied cases and is always at least as$\backslash$ntight as a bound which considers minimizing only the target error or an$\backslash$nequal weighting of source and target errors.},
author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
doi = {10.1007/s10994-009-5152-4},
file = {:home/victor/Dropbox/Mendeley/PDF/Ben-David et al. - 2010 - A theory of learning from different domains.pdf:pdf},
isbn = {9780838986219},
issn = {15730565},
journal = {Machine Learning},
keywords = {Domain adaptation,Learning theory,Sample-selection bias,Transfer learning},
number = {1-2},
pages = {151--175},
title = {{A theory of learning from different domains}},
volume = {79},
year = {2010}
}
@article{Courty2015,
abstract = {Domain adaptation from one data space (or domain) to another is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data space become more robust when confronted to data depicting the same semantic concepts (the classes), but observed by another observation system with its own specificities. Among the many strategies proposed to adapt a domain to another, finding a common representation has shown excellent properties: by finding a common representation for both domains, a single classifier can be effective in both and use labelled samples from the source domain to predict the unlabelled samples of the target domain. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labelled samples in the source domain to remain close during transport. This way, we exploit at the same time the few labeled information in the source and the unlabelled distributions observed in both domains. Experiments in toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches.},
archivePrefix = {arXiv},
arxivId = {1507.00504},
author = {Courty, Nicolas and Flamary, R{\'{e}}mi and Tuia, Devis and Rakotomamonjy, Alain},
eprint = {1507.00504},
file = {:home/victor/Dropbox/Mendeley/PDF/Courty et al. - 2015 - Optimal Transport for Domain Adaptation.pdf:pdf},
isbn = {9782875870148},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
keywords = {Machine Learning},
mendeley-tags = {Machine Learning},
number = {X},
pages = {22--24},
title = {{Optimal Transport for Domain Adaptation}},
url = {http://arxiv.org/abs/1507.00504},
volume = {X},
year = {2015}
}
@article{Knight2008,
abstract = {As long as a square nonnegative matrix A contains sufficient nonzero elements, then the Sinkhorn-Knopp algorithm can be used to balance the matrix, that is, to find a diagonal scaling of A that is doubly stochastic. It is known that the convergence is linear, and an upper bound has been given for the rate of convergence for positive matrices. In this paper we give an explicit expression for the rate of convergence for fully indecomposable matrices. We describe how balancing algorithms can be used to give a measure of web page significance. We compare the measure with some well known alternatives, including PageRank. We show that, with an appropriate modi. cation, the Sinkhorn-Knopp algorithm is a natural candidate for computing the measure on enormous data sets.},
author = {Knight, Philip A.},
doi = {10.1137/060659624},
file = {:home/victor/Dropbox/Mendeley/PDF/Knight - 2008 - The Sinkhorn-Knopp algorithm convergence and applications.pdf:pdf},
issn = {0895-4798},
journal = {SIAM J. Matrix Anal Appl.},
keywords = {Probabilities. Mathematical statistics},
number = {1},
pages = {261--275},
title = {{The Sinkhorn-Knopp algorithm: convergence and applications}},
url = {http://dx.doi.org/10.1137/060659624},
volume = {30},
year = {2008}
}
