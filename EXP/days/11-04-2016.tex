
\labday{Lundi, 11 avril 2016}
\label{day:11-04-2016}

{\Large \textbf{Vocabulaire}}

L'objectif de ces expériences est d'arriver à associer entre-elles des données
provenant de distributions très similaires.

On dispose d'un jeu de donnée \textbf{Source} qui contient les données 
\textbf{Originales}, \textbf{Non biaisées} ou encore proviennent de
la \textbf{Réalité}.

Ce jeu de donnée est opposé aux données \textbf{Cible} qui contiennent des
données \textbf{Transformées}, \textbf{biaisées} ou encore proviennent de
\textbf{Simulations}.

On cherche ici à construire un \textbf{correcteur} qui va rapprocher/projeter
les données \emph{Cibles} sur les données \emph{Sources} correspondantes.

On note $\mathcal{X_S}$ les données sources, $\mathcal{X_T}$ les données 
cibles (Target) et $\mathcal{Y}$ les labels (s'il y en a).

On note $\phi$ la fonction inconnue qui a transformé/biaisé les données \emph{Cible}.
\begin{align*}
\phi : \mathcal{X_S}  &\to \mathcal{X_T} \\
                  x &\mapsto x^\prime
\end{align*}

Et $\psi$ la fonction que notre modèle va apprendre. On espère obtenir
$\phi \circ \psi = identity$.



\experiment{Apprendre l'alignement : résumé des idées et discussions}

Je résume ici une discussion sur les problèmes et solutions sur l'algorithme
visant à apprendre l'alignement pendant l'entraînement du correcteur.

\subexperiment{Brute Force}

Commençons par rappeler l'algo \emph{brute force} que l'on va par la suite
tenter d'améliorer.

L'idée précédente était d'utiliser les classes ou les sous-classes connues
afin de tenter d'aligner les données. 

\begin{algorithm}[tb]
   \caption{Alignement aléatoire}
   \label{alg:align_random}
\begin{algorithmic}
    \State {\bfseries Entrée:} données cible $\mathcal{X_T}$, données cibles $\mathcal{X_S}$, classes $\mathcal{Y}$
    \State \textcolor{gray}{Initialisation :}
    \State $c \gets $ nombre de classe dans $\mathcal{Y}$
    \State $n \gets $ nombre d'exemple dans $\mathcal{X_T}$
    \State $NN \gets$ réseau de neurone initialisé
    \While{Convergence ou temps écoulé}
        \State \textcolor{gray}{Réorganiser aléatoirement l'alignement en respectant les classes :}
        \State $idx \gets \text{array}(n)$
        \For{$i=0$ {\bfseries to} $c$}
            \State $idx_i \gets \text{where}(\mathcal{Y}=i)$
            \State $idx_i^\prime \gets \text{shuffle}(idx_i)$
            \State $idx[idx_i]\gets idx_i^\prime$
        \EndFor
        \State $\mathcal{X_T} \gets \mathcal{X_T}[idx]$
        \State \textcolor{gray}{Entraîner le réseaux pour 1 époque :}
        \State $NN.\text{train}(\mathcal{X}, \mathcal{X_S})$
    \EndWhile
    \State {\bfseries Rendre} le réseaux
\end{algorithmic}
\end{algorithm}

Cela permet en effet de ramener des distributions `en paquet' correctement 
(figure \ref{fig:cloud_rotated_classwise})

\begin{figure}[H] % images
\centering
\subfigure[Corrected data]
    {\includegraphics[width=0.45\linewidth]{fig/11-04-2016/CloudsRotated-ClassWiseCorrector-lambda-0.00e+00-corrected_data.png}}
\hfill
\subfigure[Target Data (uncorrected)]
    {\includegraphics[width=0.45\linewidth]{fig/11-04-2016/CloudsRotated-ClassWiseCorrector-lambda-0.00e+00-target_data.png}}
\caption{Clouds - Rotated }
\label{fig:cloud_rotated_classwise}
\end{figure}


Cependant comme vu précédemment (section \ref{exp:moon_0}) dans le cas de
distribution qui ne sont pas de simples nuages de point `gentils' cette approche 
peut mener à écraser les données corrigées sur le centre des distributions.
L'idée est donc d'apprendre l'alignement afin de faire mieux que rapprocher les
données du centre des distributions \emph{Source}.

Plutôt que d'aligner aléatoirement les données entre chaque époque on va chercher
les éléments de la \emph{Source} qui correspondent le plus ou sont plus proche de 
ceux provenant de la \emph{Cible}. Il nous faut donc une distance : 
$$ \Delta : x, x^\prime \mapsto distance(x, x^\prime)$$

La première idée d'algorithme serait de d'aligner les données \emph{Cibles} avec leur
plus proche voisin dans les données \emph{Sources}. Cependant une solution triviale
alignant tous les exemples de la \emph{Cible} sur un seul et même exemple de la 
\emph{Source} est à éviter.

Pour éviter cette solution il faut forcer les exemples de la \emph{Cible} à s'aligner 
sur des exemples différents de la \emph{Source}.
On considère donc les $k$ plus proches voisins de chaque élément $x_T$ de la \emph{Cible}
Puis on l'aligne sur le plus proche voisin qui a été le moins choisis jusqu'à présent.


\begin{algorithm}[tb]
   \caption{Alignement KNN (K Nearest Neighbors)}
   \label{alg:align_KNN}
\begin{algorithmic}
    \State {\bfseries Entrée:} données cible $\mathcal{X_T}$, données cibles $\mathcal{X_S}$, classes $\mathcal{Y}$
    
    \Function{k-closest}{$x, X_S$} 
    \State $n \gets $ nombre d'exemple dans $\mathcal{X_S}$
    \State $closest \gets \text{array}(n)$
    \For{$i=0$ {\bfseries to} $n$}
        \State $closest[i] \gets \Delta(x, X_S[i])$
    \EndFor           
    \EndFunction

    \State \textcolor{gray}{Initialisation :}
    \State $c \gets $ nombre de classe dans $\mathcal{Y}$
    \State $n \gets $ nombre d'exemple dans $\mathcal{X_T}$
    \State $NN \gets$ réseau de neurone initialisé
    \While{Convergence ou temps écoulé}
        \State \textcolor{gray}{Réorganiser aléatoirement l'alignement en respectant les classes :}
        \State $idx \gets \text{array}(n)$
        \For{$i=0$ {\bfseries to} $c$}
            \State $idx_i \gets \text{where}(\mathcal{Y}=i)$
            \For{$j=0$ {\bfseries to} $n$}
                \State $closest \gets \textsc{k-closest}(\mathcal{X_T}[j], \mathcal{X_T})$
            \EndFor
            \State $idx[idx_i]\gets idx_i^\prime$
        \EndFor
        \State $\mathcal{X_T} \gets \mathcal{X_T}[idx]$
        \State \textcolor{gray}{Entraîner le réseaux pour 1 époque :}
        \State $NN.\text{train}(\mathcal{X_T}, \mathcal{X_S})$
    \EndWhile
    \State {\bfseries Rendre} le réseaux
\end{algorithmic}
\end{algorithm}


Cet algorithme est malheureusement quadratique au moment du calcul de la 
matrice de distance deux à deux entre les données \emph{Source} et \emph{Cible}.
Même si ce calcul peut être parallélisé, il reste particulièrement lourd.


\subexperiment{K-Means}

Rappel : 
On note $\phi$ la fonction inconnue qui a transformé/biaisé les données \emph{Source}
$\mathcal{X_S}$ donnant ainsi les données \emph{Cible} $\mathcal{X_T}$.
\begin{align*}
\phi : \mathcal{X_S}  &\to \mathcal{X_T} \\
                  x &\mapsto x^\prime
\end{align*}
Et $\psi$ la fonction que notre modèle va apprendre. On espère obtenir
$\psi = \phi^{-1}$.

On fait l'hypothèse (Hypothèse 1) que la \emph{Source} est composé d'une 
mixture de distributions de poids/probabilités différentes.
Si $\phi$ est `sympathique' alors la \emph{Cible} est aussi distribué en mixture

À partir d'ici on va utiliser les k-moyennes pour trouver/découper les 
distributions en clusters. Soient $z_1,...,z_k$ les clusters de la \emph{Source}
et $p_1, ..., p_k$ les clusters de la \emph{Cible}. À ces centres $z_i$ sont associés
les ensembles d'exemples $S_i$ (\emph{Source}) et aux centres $p_j$ sont associés
les ensembles $C_j$ (\emph{Cible}).

On a 3 méthodes:
\begin{enumerate}
    \item Plutôt que de calculer les distances 2 à 2 entre $\mathcal{X_S}$ et
    $\mathcal{X_T}$ on le fait entre ${z_1,...,z_k}$ et $\mathcal{X_T}$. 
    La représentation des données \emph{Source} se limite aux centres de ses 
    clusters. L'algorithme d'alignement n'est plus quadratique mais 
    $\mathcal{O} (k\times n)$.

    Cela a pour défaut de ne pas intégrer la variance des distributions dans le
    correcteur et risque d'écraser les données dans les centres.

    \item On procède en 2 étapes:
    \begin{enumerate}
        \item On calcul les distances 2 à 2 entre ${z_1,...,z_k}$ et
        ${\psi(p_1),...,\psi(p_k)}$.
        Ce qui nous donnent des ensembles $S_i$ d'exemples de la \emph{Source} à 
        associer aux ensembles $C_j$ d'exemples de la \emph{Cible} pour chaque couple 
        $z_i,p_j$ précédemment associée comme étant proche.

        \item Maintenant il reste à trouver les plus proches voisin dans chacun de ces
        sous ensemble de $X_S$ et $X_T$. C'est-à-dire de calculer les distances 2 à 2
        entre tous les éléments des $S_i$ et des $C_j$ puis de les aligner avec le k-plus
        proche voisin ayant le moins été choisis pour le moment. 
    \end{enumerate}

    \item On procède aux même étape que la méthode précédente. Cependant une fois les 
    ensembles $S_i$ et $C_j$ associés on se contente d'aligner aléatoirement leurs éléments.

\end{enumerate}


La méthode 2.a) a l'avantage de pouvoir prendre en compte la variance/les poids
des clusters dans la distance. Par exemple :
$$\Delta(z, p) = ||\psi(p) - z|| + C||card(C_i)-card(S_i)||$$
(card ou une fonction représentant le poids du culster)

L'importance d'utiliser les poids/probabilités des clusters est d'éviter que 
les solutions soient invariantes par permutation des clusters 
(figure \ref{fig:cluster_alignment}).

\begin{figure}[H] %
\centering
\subfigure[Clusters de même `taille'. Une association possible]
    {\includegraphics[width=0.45\linewidth]{fig/11-04-2016/Distribs_0.png}}
\hfill
\subfigure[Clusters de même `taille'. Une autre association possible. Pas de critère de sélection]
    {\includegraphics[width=0.45\linewidth]{fig/11-04-2016/Distribs_1.png}}

\subfigure[Clusters de `tailles' différentes. Association possible mais peut probable]
    {\includegraphics[width=0.45\linewidth]{fig/11-04-2016/Distribs_2.png}}
\hfill
\subfigure[Clusters de `tailles' différentes. Association respectant la `taille' des clusters]
    {\includegraphics[width=0.45\linewidth]{fig/11-04-2016/Distribs_3.png}}

\caption{Exemples d'alignements de clusters. La `taille' d'un cluster peut par
exemple être représenté par sa population}
\label{fig:cluster_alignment}
\end{figure}


Si l'hypothèse 1 ne tient pas, alors on utilise un argument de simplicité pour 
sélectionner (désambiguïser). L'exemple (a) est plus simple que le (b) 
(figure \ref{fig:cluster_alignment}).

Conseil de lecture :
Giorgio Patrini, Richard Nock, Paul Rivera, Tiberio Caetano, ``(Almost) No Label No Cry''

%----------------------------------------------------------------------------------------
